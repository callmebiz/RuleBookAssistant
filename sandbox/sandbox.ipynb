{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d887ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d5ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = json.load(open(\"../config/keys.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8513c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.tracing.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = keys['LANGCHAIN_API_KEY']\n",
    "os.environ['OPENAI_API_KEY'] = keys['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27421be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20768678",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentLoader:\n",
    "    \"\"\"Handles loading and processing D&D PDF rulebooks using LangChain.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Configure text splitter for chunking\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "    \n",
    "    def load_pdf(self, pdf_path):\n",
    "        \"\"\"Load a single PDF file using LangChain's loader.\"\"\"\n",
    "        print(f\"Loading {pdf_path}...\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Add source metadata\n",
    "        source_name = os.path.basename(pdf_path)\n",
    "        for doc in documents:\n",
    "            doc.metadata[\"source\"] = source_name\n",
    "            # Clean up potential page headers/footers\n",
    "            doc.page_content = self._clean_text(doc.page_content)\n",
    "        \n",
    "        print(f\"Loaded {len(documents)} pages from {source_name}\")\n",
    "        return documents\n",
    "    \n",
    "    def load_directory(self, directory_path, glob_pattern=\"**/*.pdf\"):\n",
    "        \"\"\"Load all PDFs in a directory using LangChain's DirectoryLoader.\"\"\"\n",
    "        print(f\"Loading PDFs from {directory_path}...\")\n",
    "        loader = DirectoryLoader(directory_path, glob=glob_pattern, loader_cls=PyPDFLoader)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Add additional metadata and clean text\n",
    "        for doc in documents:\n",
    "            # Clean text\n",
    "            doc.page_content = self._clean_text(doc.page_content)\n",
    "        \n",
    "        print(f\"Loaded {len(documents)} total pages from {directory_path}\")\n",
    "        return documents\n",
    "    \n",
    "    def split_documents(self, documents):\n",
    "        \"\"\"Split documents into chunks using the text splitter.\"\"\"\n",
    "        print(f\"Splitting {len(documents)} documents into chunks...\")\n",
    "        chunks = self.text_splitter.split_documents(documents)\n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Clean and normalize extracted text.\"\"\"\n",
    "        # Remove headers/footers\n",
    "        lines = text.split('\\n')\n",
    "        filtered_lines = [line for line in lines if len(line.strip()) > 1]\n",
    "        \n",
    "        # Remove excessive whitespace\n",
    "        text = '\\n'.join(filtered_lines)\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        text = re.sub(r' {2,}', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def process_pdf(self, pdf_path):\n",
    "        \"\"\"Process a single PDF: load and split into chunks.\"\"\"\n",
    "        documents = self.load_pdf(pdf_path)\n",
    "        chunks = self.split_documents(documents)\n",
    "        return chunks\n",
    "    \n",
    "    def process_directory(self, directory_path):\n",
    "        \"\"\"Process all PDFs in a directory: load and split into chunks.\"\"\"\n",
    "        documents = self.load_directory(directory_path)\n",
    "        chunks = self.split_documents(documents)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1316a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_loader = DocumentLoader(output_dir=\"../data/processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78c3733b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDFs from ../data/raw...\n",
      "Loaded 967 total pages from ../data/raw\n",
      "Splitting 967 documents into chunks...\n",
      "Created 4749 chunks\n"
     ]
    }
   ],
   "source": [
    "raw_data_path = \"../data/raw\"\n",
    "pdfs = os.listdir(raw_data_path)\n",
    "splits = doc_loader.process_directory(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bfaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=\"../data/vectorstore\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9bad0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rag\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ruleBookAssistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
